%\documentclass{acm_proc_article-sp}
\documentclass[a4,twocolumn,10pt]{article}

\usepackage[utf8x]{inputenc}

\usepackage{graphicx} 
\usepackage{subfigure}
\usepackage{paralist}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{url}
\usepackage{xspace}
\usepackage{booktabs}

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}

\usepackage[draft,nomargin,footnote]{fixme}

\graphicspath{{figs/}}

\newcommand{\eg}{\textit{e.g.}\xspace}
\newcommand{\etal}{\textit{et al.}\xspace}
\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\etc}{\textit{etc.}\xspace}
\newcommand{\vs}{\textit{vs.}\xspace}


\newcommand{\anti}{{$\mathcal{A}_0$ }}
\newcommand{\antf}{{$\mathcal{A}_1$ }}
\newcommand{\deltaant}{{ $\Delta_{\mathcal{A}_0,\mathcal{A}_1}$ }}

\title{New Insights on the Role of Context in the Perception of a Human-like Robot}

\author{
Séverin Lemaignan, Ashish Ranjan Jha, Kshitij Sharma, Pierre Dillenbourg\\
CHILI Lab, École Polytechnique Fédérale de Lausanne\\
\url{firstname.lastname@epfl.ch}
}

\begin{document}
\maketitle
\begin{abstract}

What leads us to attribute human-like characteristics to robots? The physical
appearance of the robot and the design of its behaviours do obviously play a key
role. This paper however evidences another mechanism at play, more subtle and partially
unconscious: the context of the interaction, and more precisely, the cognitive
prerequisites that the interaction presupposes.

To study this effect, we propose an original methodology based on eye-tracking
and a serie of stimuli that are visually identical and yet elicit different
assumptions regarding the cognitive capabilities of the robot. We then correlate
the gaze patterns with established questionnaires that assess anthropomorphic
attributions.

\begin{inparaenum}[\itshape a\upshape)]
As a result, we show that \item specific gaze patterns (fixations on the head of
the robot) correlate with post-hoc anthropomorphic
attributions, and hence, eye-tracking can be used as a novel \emph{in-the-moment}
metric of anthropomorphism, and \item simple context priming is sufficient
to elicit significantly different anthropomorphic attributions.
\end{inparaenum}

This second point has important implications regarding the design of human-like human-robot
interactions, and we propose to discuss them at the end of the article.

%We present a study that investigates the relation between anthropomorphism and
%gaze patterns of the observers. Anthropomorphism usually refers to attributing
%human-like characteristics by a human to a non-human agent/entity; in this
%study, the non-human entity is a robot. 
%
%We hypothesize that while observing a human-robot interaction in a particular
%setting, the observer develops an anthropomorphic behavior for the robot.
%Moreover, the resulting anthropomorphism can be correlated with the gaze
%patterns of the observer as well as with the recorded response of the
%participants about their experience of the interaction (in the form of a
%questionnaire), both before and after the experiment. We try to seek the answer
%to the question whether anthropomorphism goes beyond the shape and body
%features, and we indeed observe that just by the use of audio commands of
%different cognitive levels, anthropomorphic attitude gets affected. We obtain
%two other useful results. First, eye tracking serves to be an effective way for
%measuring anthropomorphism. Second, in our study, the standard anthropomorphism
%tests are also validated such as, a person who can anthropomorphize more, in
%general, responds to a human scene and a robot scene in a similar manner.
%
%We observe a significant increase in the gaze fixations over the actor's head
%for a high cognitive scene compared to a low cognitive scene. We also achieve
%the result that the high cognitive interaction scenarios are capable of inducing
%anthropomorphic behavior more than the low cognitive scenarios.

\end{abstract}

%\keywords{anthropomorphism, eye tracking, gaze patterns, robot}

\section{Introduction}

Robotics researchers often tend to consider that anthropomorphism only describes
a static set of human-like features of a robot (like its shape, its speech
capabilities, facial expressions, etc.). We refer to these characteristics as
the \emph{anthropomorphic design} of the
robot~\cite{fink_anthropomorphism_2012}, and propose that
\emph{anthropomorphism} in general refers to a \emph{social phenomenon} that
emerges from the (real or imagined) interaction between a non-human agent (a
robot in our case) and a human~\cite{persson_anthropomorphism_2000}.  According
to~\cite{epley_when_2008}, anthropomorphism includes the human perception of
emotional states, motivations, intentions in the non-human agent, and tends to
ascribe those qualities to it. As such, the \textbf{dynamic and socio-cognitive
dimensions of anthropomorphism are essential to understand the complex bonds
that human users build with artificial agents such as
robots}~\cite{lemaignan2014dynamics}, and, even further, we postulate that
anthropomorphism can therefore be considered as a useful \emph{proxy} to these
intricate psychological bonds. This article aims at advancing our understanding
of this phenomenon \textbf{by evidencing the impact of the cognitive
presuppositions implied by the context of the interaction}.

The study of context in HRI is not new. \textit{Context of use} is defined by
the ISO as \textit{``users, tasks, equipment (hardware, software and materials),
and the physical and social environments in which a product is used''}, and
accordingly, there are various aspects that can impact how a robot is
experienced. The real or imagined \textit{purpose} of a robot, including the
application context in which it is typically used (\eg office environment, home,
rehabilitation center, public space, \etc) as well as the task context (\eg
serious or playful task; composition of human-robot team; number of users) and
the (social) role in which the robot is used / experienced are taken into
account. Generally, when the context of use of a robot is social, entertaining
or playful, it will enhance anthropomorphism, compared to when the context is a
routine or focused serious task (security, rescue, \etc).
\cite{joosse_what_2013} showed, for instance, that when the same robot ({\sc
nao}) was used in a different task context (cleaning task \vs tour guide), users
ascribed different personalities to the robot. \cite{goetz_cooperation_2002}
revealed a link between a robot's context of use and people's perception of the
robot: the authors found that people prefer a serious robot for serious tasks
and a less serious robot for more playful tasks. \cite{kaplan_free_2000}
discussed the role of uselessness in the design of robots. The author argues
that artificial pets like AIBO have no real purpose, in a sense that they do not
provide any kind of service or, in other words, \textit{``they are not doing
what you tell them to do''} and \textit{``they might refuse the order of its
owner''}. It may be this very aspect that increases people's tendency to
anthropomorphize the robot. According to Kaplan, it is in our daily use language
that we tend to attribute intentions to devices that are not doing their job
well.

These previous studies investigated users' perception of robots in
different prototyped tasks (routine vs playful, typically boring vs typically
intellectual) and their results nicely supported our general intuition.
In this article, we show that much more subtle variations in the task context do
lead to very noticeable effects as well.

Measuring anthropomorphism raises interesting methodological challenges, if only
tbecause people's perception of robots change over the duration of their
interaction with the robots. Takayama discusses the effects of this
change~\cite{takayama_perspectives_2012} and differentiates between what she calls an
\emph{in-the-moment} and a \emph{reflective} perspective on agency: an
\emph{in-the-moment} perspective would refer to one's most immediate
response/sense in a given situation, while a \emph{reflective}
perspective would indicate one's reaction/sense of a situation based on a more
in-depth  consideration and contemplation.

Quite often, these two distinctions are not considered differently, leading to
errors and confusions.  In other words, in an initial phase of interaction with
a non-human agent, people might respond ``mindlessly'' instead of responding
consciously~\cite{nass_machines_2000}. Only post a due amount of
time of, what is generally called as ``familiarization'' with the robot, one
might respond in a more reflective manner instead of an in-the-moment reaction.
This can be demonstrated by the fact that when participants who had interacted
with a technology, were asked about whether the interaction with the technology
was in a human-like way, most of them denied this which shows the reflective
perspective . However, the same participants had actually reacted ( in the
moment of the interaction) to the system in many ways that were quite close to
how they would react to people~\cite{reeves_media_1996}.

Most of the experiments that investigate anthropomorphism in HRI are conducted
in controlled lab-settings during a short-term scenario. For instance, pictures
or videos of different robots are shown to human subjects who subsequently fill
in a questionnaire, to assess how far the different types of robots were
perceived human-like. Such experimental settings have limitations for studying a
social phenomenon like anthropomorphism, and one may wonder if anthropomorphism
can really be measured in a post-interaction closed questionnaire or by rating
scales. Findings might possibly lead to interpretations that would not be
generalizable to natural settings or real interaction scenarios (which is always
an issue in social science research).

The Godspeed questionnaire~\citep{bartneck_measurement_2008}, and in particular,
its subpart focused on anthropomorphism, is the main validated questionnaire to
assess anthropomorphism. On 5 point semantic differential scales, people are
asked to rate the following constructs: fake \vs natural, machinelike \vs
humanlike, unconscious \vs conscious, artificial \vs lifelike, moving rigidly
\vs moving elegantly. Because the concept of ``human-likeness'' itself is
complex and abstract, \cite{kahn_jr._robotic_2006} suggest to ask for more
concrete constructs that are typical or unique of the concept of
``human-likeness'', and \cite{ruijten_introducing_2014} propose for instance a
25-item questionnaire to measure various concrete aspects of human-likeness.



\subsection{Hypotheses}

There were four hypotheses made in this experiment\fxwarning{need to change
order of hypotheses(as Kshitij suggested) but the flow breaks by placing H4
first, H1 last}. We hypothesized (\textbf{H1}) that the gaze patterns can
distinguish between human and robot interaction scenarios. Further, this gaze
pattern difference can be denoted as the distribution of gaze on areas of
interest in the scene.  Secondly, we hypothesized (\textbf{H2}) that the
difference in gaze patterns between human and robot conditions
($\delta_{\mathcal{H},\mathcal{R}}$) correlate with the participants'
\textit{initial tendency to anthropomorphize} (\anti), where \anti was measured
as the sum of the ratings (responses) given by the participants to the questions
of the pre-questionnaire. This means that for participants whose \anti is low,
the gaze patterns should be significantly different for the robot videos
compared to the human videos.

We also hypothesized (\textbf{H3}) that the gaze patterns can distinguish
between high-cognitive and low-cognitive tasks. Finally, we hypothesized
(\textbf{H4}) that the cognitive priming will have an effect on the difference
between \anti and \textit{final tendency to anthropomorphize} (\antf) \ie
\deltaant (where \antf is measured for the post-questionnaire in a similar way
as the \anti).

\fxwarning{This whole para already being discussed under Experimental Design.
Can remove this para}We used stationary eye tracking technique for tracking the
gaze patterns and ``Nao", a robot manufactured by Aldebaran Robotics was used in
this experiment. The interactions were recorded as videos and were then shown to
the participants.  The scenarios that were covered in the videos were aimed at
eliciting the human-like or high-cognitive (HC) feelings for the robot in one
setting and robot-like or low-cognitive (LC) feelings in the other setting. We
tried to keep our distribution of participants uniform across all variations of
the scenarios to avoid any biases to our results. The change in the
feelings\fxwarning{can we have a more specific word than 'feeling'?} of
participants were recorded in the form of response to two questionnaires before
and after the experiment. The difference in the response of the two
questionnaires was used to measure the impact the videos had on the
participants. 

\fxwarning{This para is here just to justify why 3rd scenario isn't used. Else
the contents already being discussed in sections 3.1, 3.2}As mentioned earlier,
the human element in the robot interaction was in the form of a command given by
the human to the robot. This command (\eg ``pick up the brown toy") was used to
prime the context thereby classifying the scenario as LC or HC. Initially we had
three different scenarios where the robot/human was asked:

\begin{enumerate}
    \item to pick up a toy
    \item to point to a sound/noise
    \item to show some movements (or dance)
\end{enumerate}

\fxwarning{Is Introduction the best place for justification for not using 3rd
scenario?}Differing from first two scenarios, the third scenario had only one
object in the video \ie~the actor (human or robot). In the first two scenarios,
a participant could make gaze transitions, say, between the robot and the toys
for the first scene. In contrast, in the third scene, a participant could only
look at the robot showing its movements. The third scene, hence, was more
related to observing the various body movements and was not helpful in
identifying differences in gaze patterns based on LC and HC scenarios and
therefore, we did not consider it for our study.

\subsection{Novel Contributions}

The study and results presented in this article lead to two main contributions:
new insights on the impact of the cognitive context of the interaction on the
perception of a robot by humans; a novel unbiased\fxwarning{correct?}
methodology based on eye-tracking to measure anthropomorphic attributions during
a running human-robot interaction.

Our first contribution extends our understanding of the attribution of
human-like characteristics to robot by exploring the role of the interaction
context. As we will show in the article, it appears that \textbf{even relatively
subtle context priming may lead to significantly different anthropomorphic
perceptions of the exact same robot, performing the exact same task}, as
confirmed both by distinct gaze patterns, and different reported perceptions in
questionnaires. As far as we know, this effect had not been previously
evidenced.

This leads to a second-order effect that we also evidence here: depending on the
cognitive context, the tendency to attribute human-like characteristics to robot
\textbf{evolves} in significantly different ways: after observing a robot in a
context that pre-suppose deeper cognitive capabilities, \textbf{people increase
their general tendency to anthropomorphize robots} compared to a shallow
cognitive context, even though the robot appearance and behaviour is the exact
same.

The second contribution is a methodological one: we present in this article
\textbf{a novel technique to assess anthropomorphic projections that relies on a
biometric measurement (eye-tracking) to compare gaze fixation durations on
the face of the robot}. We cross-validate this new metric with existing,
established, questionnaires. Compared to current techniques (post-hoc
annotations of videos and questionnaires), this new approach is objective,
is less impacted by experimental biases (like the \emph{observer effect},
where the subject's behaviour is impacted by the fact he/she knows that
he/she is observed) and take place during the interaction itself
(\emph{in-the-moment} measurement).

Besides, we also introduce a new kind of visual stimuli, specifically designed
to study the impact of non-appearance, non-behavioural related effects on the
human-like perception of robots. We make these video stimuli available to the
community, and therefore invite our colleagues to reproduce our experimental
results.


As a whole, we believe that this article provide a solid contribution, both in
terms of methodology and experimental evidence, to our understanding of one of
the intricate psychological effects that come to play when humans and robots
interact.

\section{Experimental Design}

This study is build as an eye-tracking experiment (mainly measuring fixation
times on various body parts) where participants watch short videos of an agent
(either a robot or a human) performing very simple task (picking an object or
pointing toward a sound source), in two different socio-cognitive contexts.
These contexts, created by priming and/or audio cues, are designed to elicit
either a shallow cognitive context (that participant would refer as a
\emph{machine-like} situation) or a deeper cognitive context (refered as a
\emph{human-like} situation).

\subsection{Conditions}

The study follows a 2x2 design, summarized in table~\ref{table:design}. Our two
independent variables are the level of socio-cognitive context (\emph{shallow
cognitive context} vs \emph{deep cognitive context}) and the nature of the
agent appearing in the stimulus (a \emph{robot}, eliciting a \emph{human-robot} interaction
situation, or a \emph{human}, eliciting a \emph{human-human} interaction
situation).

\begin{table}
    \centering
    \small

    \tikzmark{top}

    \tikzmark{left}
    \begin{tabular}{l|p{2.5cm}|p{2.5cm}}
        & \tikzmark{top1} Shallow cognitive context & Deep cognitive \tikzmark{top2} context  \\
        \hline
        \tikzmark{left1} Robot & {\bf A}: \emph{``Pick the brown toy''} & {\bf A}: \emph{``Pick your favorite toy''} \\
                               & {\bf B}: \emph{``Point at the noise''} & {\bf B}: \emph{``Point at the crying baby''} \\
        \hline
        \tikzmark{left2} Human & {\bf A}: \emph{``Pick the brown toy''} & {\bf A}: \emph{``Pick your favorite toy''} \\ 
                               & {\bf B}: \emph{``Point at the noise''} & {\bf B}: \emph{``Point at the crying baby''}\tikzmark{bottom} \\
        \end{tabular}

    % draw the over- and side-braces
    \begin{tikzpicture}[overlay, remember picture]

        \draw [decoration={brace,amplitude=0.5em},decorate,thick]
            (top -| top1.west) --  (top -| top2.east) node[midway, above=0.5em] {\scriptsize between subjects};

        \draw [decoration={brace,amplitude=0.5em},decorate,thick]
            (left |- bottom) --  (left |- left1.north) node[midway,above=3em,left=1em,rotate=90] {\scriptsize within subject};
    \end{tikzpicture}
    %%%

    \caption{\small The study follows a 2x2 design: the \emph{robot} vs
        \emph{human} condition is within subject, while the \emph{shallow
        cognitive context} vs \emph{deep cognitive context} is between subject.
        Two stimuli (\emph{Picking} task, {\bf A}, and \emph{Sound} task,
        {\bf B}) were shown to the participants, introduced by brief verbal
        commands, reproduced in the table. While visually identical (see
        screenshots on figure~\ref{fig:stimuli}), the stimuli were introduced
        with different commands in the two conditions \emph{shallow} and
        \emph{deep cognitive
        context}.}

    \label{table:design}
\end{table}


\subsection{Video Stimuli}

Four different video stimuli where filmed (figure~\ref{fig:stimuli}): two
different tasks (picking a stuffed animal, refered hereafter as the
\emph{Picking} task, and pointing towards a source of sound, refered as the
\emph{Sound} task) acted either by a human or a robot. Video stimuli were
realised in studio conditions. All videos followed the same simple structure: an
initial audio command (spoken by an invisible person), followed by the task
being executed.

The human actor was instructed to follow as closely as possible the actions and
attitudes of the robot (left/right glances, hesitations\ldots), while keeping a
natural, \emph{human-like} general behaviour.  Hence, the length of human videos
(57 seconds in total for the two tasks) was shorter than robot videos (110
seconds) due to the robot being usually slower in performing actions (in
particular walks) compared to human.

The audio (and audio only) of these four videos was edited to create two sets of
stimuli: one for the \emph{shallow cognitive context} condition, one for the
\emph{deep cognitive context} condition. The audio editing consisted in
inserting \emph{different commands} to initiate the tasks (reported in
table~\ref{table:design}), and, in the \emph{Sound} task, to use as sound source
either a repetitive bip (\emph{shallow cognitive context}) or the sound of a
crying baby (\emph{deep cognitive context}).

\begin{figure}
    \centering
    \subfigure[\emph{Picking} task, robot condition]{
        \includegraphics[height=3cm]{stimulus-robot-toys}
    }
    \subfigure[\emph{Picking} task, human condition]{
        \includegraphics[height=3cm]{stimulus-human-toys}
    }

    \subfigure[\emph{Sound} task, robot condition]{
        \includegraphics[height=3cm]{stimulus-robot-noise}
    }
    \subfigure[\emph{Sound} task, human condition]{
        \includegraphics[height=3cm]{stimulus-human-noise}
    }


    \caption{\small Screenshots of the 2x2 stimuli. Note that the images above have been
    slightly cropped: the original video are framed so that the agent (robot or
    human) always remains entirely visible.}
    \label{fig:stimuli}
\end{figure}

\subsection{Questionnaires}

Before starting the experiment, the participants were asked to fill a
questionnaire which had 49 questions. The responses to the questions were scaled
in the 5-point Likert scale, numerically ranging from -2 to 2. The questionnaire
had ten questions from the 10 item big-five personality questionnaire. And then
there were questions taken from Godspeed questionnaire and other
questionnaires. In this report, we refer to this questionnaire as the
pre-questionnaire \ie before the experiment, and it typically took 5 minutes for
participants to fill it. 

A second questionnaire was administered at the end of the experiment, that was
almost identical to the pre-questionnaire.  This questionnaire typically took 3
to 4 minutes to fill and we refer to this as the post-questionnaire.

The difference (noted \deltaant, computation detailed below) observed for
each participant between the pre- and post-questionnaires form the dependent
variable of our experiment. It reflects the impact of the video stimuli on the
anthropomorphic perception that they have of robots \emph{in general}: the
Godspeed questions used in the questionnaires are not specific to the Nao robot,
and assess in a broader way how humans perceive robots.

\section{Experiment}

\subsection{Course of the study}

\subsubsection{Participants}

We did a pilot test using 10 participants first, and then after some
modifications, we did the final experiment on 40 participants.

Participants on an average were around 21 years of age (SD=4.5). Among 40
participants, 21 were females and 19 were males.

The participants were recruited amongst students of our university. To avoid students
with a higher familiarity with robots and artificial intelligent systems,
Computer Science and Electronics curriculum were excluded. This was \textit{a
posteriori} comforted as per the questionnaire responses: no
participants\fxwarning{to be checked : actually 5 out of 40 reported to be familiar(they marked +1 or +2 in questionnaire)}
reported itself as being familiar with robots and only a few\fxwarning{how
many? ( = 8 out of 40 )} out of 40 had ever owned a robot.

It took
approximately 15 minutes per participant for one experiment session.  Each
participant was invited for only one session. 

The \emph{shallow} vs \emph{deep cognitive context} condition was between
subject: Out of the 40 participants, 22 watched the \emph{deep cognitive}
tasks and 18 watched the \emph{shallow cognitive} tasks.

The \emph{robot} vs \emph{human} condition was within subject. The order of
stimuli presentation was counter-balanced over the 40 participants.

\subsubsection{Pre-questionnaire and familiarization}

\fxwarning{Was it first the pre-questionnaire or the familiarization?}
Before the experiment, participants were made to read, understand and sign a
standard consent form. Participants were then asked to fill the 49 questions of
the pre-questionnaire, without time constraint\fxwarning{right?}.

After the pre questionnaire, we invited the participants to an initial
free interaction with a \textbf{powered-off} Nao robot (we did not constraint
the time, and participants played between 2 and 3 minutes ith the robot).

The purpose of this pre-interaction was to get the participants acquainted with the
robot shape and kind of motions it could possibly perform, and therefore
mitigate the (visual) novelty effect (so that they do not get surprised seeing
a robot for the first time in the videos).

\subsubsection{Videos}

After filling the pre-questionnaire, we conducted a brief eye-tracker
calibration procedure, and the participants' gaze was recorded while they
watched the \emph{robot} and \emph{human} interaction videos.

\subsubsection{Post-questionnaire}

Finally, the participants were asked to fill the post-questionnaire.

Before leaving, each of the participants were given a reward equivalent to CHF
10.

\subsection{Data Collection}

\subsubsection{Gaze variables}

\paragraph{Areas Of Interest(AOI)}

Among the two scenarios, the first was one was about the robot/human picking up
a toy and the second one about pointing to the noise. As seen in
figure~\ref{fig:aoi-toys}, in
the first scenario, we had 10 AOIs: 1 for the actor's (robot/human) head, 2 for
arms (left and right), 2 for hands, 2 for legs and 1 for torso, and also 2 for
the two toys (green and brown). Similarly, as shown in
figure~\ref{fig:aoi-noise}, we had 10
AOIs for the second scenario, where the 2 AOIs for toys were replaced by two
speakers that produced sound. We fetched the gaze patterns that fall into one of
these 10 AOIs and analyzed the gaze distribution among them.

\begin{figure}
    \centering
    \subfigure[\emph{Picking} task, robot condition]
    {\includegraphics[width=0.8\columnwidth]{aoi-robot}\label{fig:aoi-toys}}

    \subfigure[\emph{Sound} task, human condition]
    {\includegraphics[width=0.8\columnwidth]{aoi-human}\label{fig:aoi-noise}}

    \caption{Areas of interest used for the gaze analysis}
    \label{fig:aoi}
\end{figure}

\paragraph{Proportion of time on the AOIs}

We fetch only the gaze patterns that fall into one of these 10 AOIs and analyze
the gaze distribution among them based on the proportion of time spent in
viewing each of these AOIs. This helps in drawing comparisons between various
objects present in the scene. 

\paragraph{Difference in gaze patterns}

An important data to be obtained here is $\delta_{H,R}$. This is calculated by
taking the difference between the ratio of net dwell time of an AOI and the AOI
fixation coverage summed over all the AOIs in the scene. Hence,
$\delta_{\mathcal{H},\mathcal{R}}$ is expressed as a ratio in the range of 0 to
1. Similarly, the differences between gaze patterns of two participants, where
one watches the LC task, given by $\delta_{low}^{\mathcal{H},\mathcal{R}}$ and
the other watches the HC task, given by
$\delta_{high}^{\mathcal{H},\mathcal{R}}$ are important gaze variables.

\subsubsection{Questionnaires processing}

\fxnote{TDB}

Questionnaires were coded and two values, the \emph{initial tendency to
anthropomorphize} \anti and the \emph{final tendency to anthropomorphize} \antf
were computed, respectively from the pre- and post-questionnaires.

These values were computed by a direct sum of the rating provided by the
participants for the Godspeed's questions in the category
\emph{Anthropomorphism} and \emph{\fxwarning{?}...}. These ten questions were
rated between -2 and 2, hence \anti and \antf vary between -20 and 20.

The other questions (including the Big-Five personality test) were not used for
this study.

\section{Results}

Based on the experiments conducted on 40 participants in the $\mathcal{H}$ and
robot interactions across LC and HC conditions, we have the following
results. (The detailed statistics and code used for obtaining the results are
available here : \url{https://github.com/chili-epfl/anthropomorphism-eyetracking})

\subsection{General Biases}

While testing for all the 4 hypotheses, we observed no significant bias with
respect to age, gender, EPFL status, familiarity with robot and robot ownership
of the participants, \ie there was no correlation found between these factors
and the gaze patterns in the videos.

\subsection{Manipulation check}

\begin{figure}
    {\includegraphics[width=3.3in]{ManipulationCheck}}
    \caption{Manipulation check}
    \label{fig:ManipulationCheck}
\end{figure}


In our post-questionnaire, we had a question asking the participants about
whether the video has a robot kind of task or a human kind of task on a scale of
-2 to 2 where -2 refers to robot kind of task. As can be seen in figure 2, we
got a significant correlation in this manipulation check (F[1,36] = 20.42, p <
.01) which means that the participants that were shown the LC videos identified
the task as the robot-like and participants watching HC videos perceived the
tasks as more human-like. 




\subsection{H1: Gaze patterns for Human vs Robot}

Among the 10 AOIs, we grouped the AOIs corresponding to the actor into 4
categories : Head, Arm (containing both arms and hands), Torso, Leg(containing
both legs). And this is the result that we obtained from the ANOVA between gaze
patterns on the AOIs versus the human and robot videos :

\textit{\textbf{head}}: the fixations on the head for the human video
were observed to more than that for the robot video (F[1,36] = \ 6.60, p
< .05)

\textit{\textbf{arm}}\ : the fixations on the arms for the $\mathcal{H}$ video
were also greater than that for the robot video (F[1,36] = 18.65, p <
.01)

\textit{\textbf{leg}}\hspace{0.18cm} : the fixations on the legs were much lower
for the human video as compared to the robot video (F[1,36] = \
2.89, p < .1)

\textit{\textbf{torso}} : the fixations on the torso was more for the
human video than for the robot video (F[1,36] = \ 3.54, p < .1) 

\subsection{H2: Difference in gaze patterns vs \anti}

We got a significant correlation between $\delta_{\mathcal{H},\mathcal{R}}$ and
the \anti. As can be seen in figure 3(a), there is a significant negative
correlation between the two quantities (Pearson Correlation Coefficient = -0.42,
p < .01).

\begin{figure}
    \subfigure[Difference in gaze patterns vs \anti]
    {\includegraphics[width=3.3in]{H2}\label{GazeDifference-vs-ICA}}
    \subfigure[Gaze patterns for high vs low condition]
    {\includegraphics[width=3.3in]{GazeHighLow}\label{GazeHighLow}}
    \subfigure[Difference between \anti and \antf versus High/Low]
    {\includegraphics[width=3.3in]{H4}\label{ICAtoAAPImprovement}}
    \caption{Hypotheses H2, H3, H4}
\end{figure}

\subsection{H3: Gaze patterns for high vs low cognitive condition}

This test was done only for robot scenario as the human case is
by default a source of HC condition, hence, would not make a big impact in
changing the patterns across the LC and HC conditions. Based on ANOVA results,
taking just the 5 AOI groups (head, arms, hands, torso, legs) for the robot in
both LC and HC conditions, the fixations on the head in the HC condition was
found to be significantly higher than that in the LC condition (F[1,36] = 4.55,
p < .05). For all other AOI groups, the fixations across LC and HC were almost
the same. Figure 3(b) shows the distribution of proportion of time spent gazing
on the 5 AOIs for LC and HC conditions.

\subsection{H4: Difference between \anti and \antf versus High/Low}

ANOVA between \deltaant and the category of cognitive task
\ie HC or LC tells us that \deltaant is greater for HC tasks
than that for the LC tasks (F[1,36] = 6.54, p < .05). The results are shown in
figure 3(c).


\section{Discussion}

Based on the results that we obtained, we can now verify our four hypotheses.
For the first hypothesis where we state that the gaze patterns can distinguish
between human and robot interaction scenarios, we get greater
amount of fixation on the head, arms and torso for the human scenario as
compared to the robot scenario. This can be explained by the fact that
human is by default perceived as a high cognitive agent compared to a robot.
Hence, the proportion of time spent on looking at the head is more in case of
human videos as compared to robot videos. But the trend is opposite for legs.
This might be explained by the fact that the participants are fascinated by the
joints on the robot's legs \ie the novelty effect. Overall, this
result weakly validates our hypothesis.

In the second hypothesis, where we claim that $\delta_{\mathcal{H},\mathcal{R}}$
should correlate with the participants' \anti, we obtain in our results, a
significant negative correlation between the two quantities. From the negative
correlation, we can understand that if \anti is high, it means that the
\textit{human likeliness ascription} (HLA) is high (which means that there is a
greater tendency to anthropomorphize) which indicates that
$\delta_{\mathcal{H},\mathcal{R}}$ should be low. This supports our hypothesis
and can be explained by the fact that if HLA is high for a participant, then
even the robot videos are quite human-like for such participant. 

The third hypothesis where we state that the gaze patterns can distinguish
between HC and LC tasks, can also be supported by our results. As can be seen in
figure 3(b), the proportion of time spent by participants on looking at the head
is significantly higher in HC task as compared to LC task. When participants
ascribe more anthropomorphic features to robot, they look more at the head than
when they ascribe less anthropomorphic features.

Finally for the fourth hypothesis where we state that the cognitive priming will
have an effect on \deltaant, as seen in the results, \deltaant
for participants that watched the HC task is greater than that for the
participants who watched LC task. This proves the fact that HLA was induced by
the videos and in fact, the HC tasks impose a greater HLA on the minds of
participants than the LC tasks. This shows the typical priming effect of the
audio command given in the beginning of the video and highlights the variation
in HLA based on the interaction scenario (LC versus HC) thereby supporting our
hypothesis. 

We analyze the gaze distribution in this study only based on the AOI net dwell
times . Apart from this, the gaze transition among AOIs can be another way of
fetching gaze patterns which is not used in this study.

\section{Conclusion}

In our experiment, we created two kinds of scenarios each for human and
robot interaction and just by changing the audio commands (priming
effect), we replicated these videos to create a HC and a LC scene. We recorded
the gaze patterns of the 40 participants who were made to watch these videos and
also the participants were asked to fill two questionnaires before and after the
experiment. These data were analyzed and compared to our four hypotheses. We
hypothesized that one who anthropomorphizes more would have a similar reaction
to human and robot conditions, and also that in the HC case, a
participant's fixations would be more on the head of the actor as compared to
that in the LC case. We also hypothesized that the HC scene would induce more
anthropomorphic attitude than the LC scene. We found that the results were
successful and supported our hypotheses. We also had another scenario where the
robot/human was asked to move/dance, but this scenario had only one object
(\ie the actor) showing movements, and hence participants would focus
more on seeing the movements leaving behind the cognition aspect, which made it
difficult to differentiate between the LC and HC cases and, therefore, we
discarded this scenario from our analysis.

\subsection{Context and Interaction Design}

\fxwarning{discuss here how context can be used to shape the interaction}

\bibliographystyle{abbrv}
\bibliography{biblio}

%\appendix
%\includepdf[pages={1,2}]{pre-questionnaire.pdf}
%\includepdf[pages={1,2}]{post-questionnaire.pdf}

\end{document}
